{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.randn(1, 5) for _ in range(4)]  # make a sequence of length 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __4__ inputs, each input has __5__ features, so LSTM __input dim = 5__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0786,  0.5960,  2.1105, -0.3480,  1.2661]]),\n",
       " tensor([[ 0.1282, -0.3206,  1.8067,  0.9972, -0.4360]]),\n",
       " tensor([[-0.6857, -1.5372, -0.7193,  0.3391, -0.7413]]),\n",
       " tensor([[-0.6050, -0.5376,  0.3297,  2.1191,  0.4590]])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape = torch.cat(inputs).view(len(inputs),1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0786,  0.5960,  2.1105, -0.3480,  1.2661]],\n",
       " \n",
       "         [[ 0.1282, -0.3206,  1.8067,  0.9972, -0.4360]],\n",
       " \n",
       "         [[-0.6857, -1.5372, -0.7193,  0.3391, -0.7413]],\n",
       " \n",
       "         [[-0.6050, -0.5376,  0.3297,  2.1191,  0.4590]]]),\n",
       " torch.Size([4, 1, 5]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape, reshape.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 is hidden_size, is hidden state dim.\n",
    "\n",
    "In this case, \n",
    "\n",
    "for each second, \n",
    "\n",
    "### xt.shape = (features, 1) => (5, 1)\n",
    "\n",
    "### yt.shape = (features, 1) => (5, 1)\n",
    "\n",
    "### ht.shape = (hidden_size, 1) => (3 ,1) \n",
    "\n",
    "### whh.shape = (hidden_size, hidden_size) =>(3,3)\n",
    "\n",
    "### wxh.shape = (hidden_size, features) => (3,5)\n",
    "\n",
    "### why.shape = (features, hidden_size) => (5,3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For each element in the input sequence, each layer computes the following function__\n",
    "\n",
    "\\begin{split}\\begin{array}{ll}\n",
    "i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
    "c_t = f_t c_{(t-1)} + i_t g_t \\\\\n",
    "h_t = o_t \\tanh(c_t)\n",
    "\\end{array}\\end{split}\n",
    "\n",
    "![1.png](lstm-img/lstm1.png)\n",
    "![2.png](lstm-img/lstm2.png)\n",
    "![3.png](lstm-img/lstm3.png)\n",
    "![4.png](lstm-img/lstm4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(5,3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __input_size__ – The number of expected features in the input x\n",
    "* __hidden_size__ – The number of features in the hidden state h\n",
    "* __num_layers__ – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1 \n",
    "\n",
    "(__this is how many layers stack upper together__)\n",
    "\n",
    "* __bias__ – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "* __batch_first__ – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "* __dropout__ – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "* __bidirectional__ – If True, becomes a bidirectional LSTM. Default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1: initialize hidden\n",
    "hidden is tuple, \n",
    "(h0,c0)\n",
    "In this case, both h0 and c0 are (2,1,3)\n",
    "\n",
    "                                    (\n",
    "                                    \n",
    "                                     2  => lstm_num_layer,\n",
    "                                     1  => mini-batch_size,\n",
    "                                     3  => hidden_size\n",
    "                                     \n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = (torch.randn(2, 1, 3),\n",
    "          torch.randn(2, 1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2: construct input\n",
    "input of lstm is (5,1,10)\n",
    "\n",
    "                 (\n",
    "                 \n",
    "                  4 => sequence_num,\n",
    "                  1 => mini-batch_size,\n",
    "                  5 => features number for every time\n",
    "                  \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 5])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = torch.cat(inputs).view(len(inputs),1,-1)\n",
    "input1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before inputs is:\n",
      " [tensor([[ 0.0786,  0.5960,  2.1105, -0.3480,  1.2661]]), tensor([[ 0.1282, -0.3206,  1.8067,  0.9972, -0.4360]]), tensor([[-0.6857, -1.5372, -0.7193,  0.3391, -0.7413]]), tensor([[-0.6050, -0.5376,  0.3297,  2.1191,  0.4590]])]\n",
      "\n",
      "before length of inputs is: 4\n",
      "\n",
      "after input1 is:\n",
      " tensor([[[ 0.0786,  0.5960,  2.1105, -0.3480,  1.2661]],\n",
      "\n",
      "        [[ 0.1282, -0.3206,  1.8067,  0.9972, -0.4360]],\n",
      "\n",
      "        [[-0.6857, -1.5372, -0.7193,  0.3391, -0.7413]],\n",
      "\n",
      "        [[-0.6050, -0.5376,  0.3297,  2.1191,  0.4590]]])\n",
      "\n",
      "after input1.size is: torch.Size([4, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"before inputs is:\\n {}\\n\\n\"\n",
    "      \"before length of inputs is: {}\\n\\n\"\n",
    "      \"after input1 is:\\n {}\\n\\n\"\n",
    "      \"after input1.size is: {}\".format(inputs, len(inputs), input1, input1.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input of lstm is:\n",
    "\n",
    "__input1, (h0, c0)__\n",
    "\n",
    "### Output of lstm is:\n",
    "\n",
    "__output, (hn, cn)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0829,  0.0525,  0.3308]]]) (tensor([[[ 0.0829,  0.0525,  0.3308]]]), tensor([[[ 0.1171,  0.5475,  1.0197]]]))\n",
      "tensor([[[ 0.3024,  0.6301, -0.0483]]]) (tensor([[[ 0.3024,  0.6301, -0.0483]]]), tensor([[[ 0.7309,  1.2399, -0.1758]]]))\n",
      "tensor([[[ 0.1797,  0.2369,  0.1919]]]) (tensor([[[ 0.1797,  0.2369,  0.1919]]]), tensor([[[ 0.4599,  1.5573,  0.2789]]]))\n",
      "tensor([[[ 0.1223,  0.1286,  0.1049]]]) (tensor([[[ 0.1223,  0.1286,  0.1049]]]), tensor([[[ 0.1961,  1.3332,  0.4209]]]))\n",
      "tensor([[[ 0.4101,  0.5140, -0.1696]]]) (tensor([[[ 0.4101,  0.5140, -0.1696]]]), tensor([[[ 0.4990,  0.7224, -0.2178]]]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    # hidden is a tuple\n",
    "    \n",
    "\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7789, -2.6498,  1.9009, -1.2295,  0.0009, -1.2562, -0.6465,\n",
       "           1.2039,  0.4597,  0.2007]],\n",
       "\n",
       "        [[-0.8459,  1.1001,  1.5433,  0.3010, -1.3644, -0.7850, -1.0751,\n",
       "          -0.8574,  1.8880, -0.9413]],\n",
       "\n",
       "        [[ 0.0915,  0.1007, -0.1625,  0.5021, -0.0464, -0.7785, -0.8695,\n",
       "          -0.2019, -0.5868,  0.0673]],\n",
       "\n",
       "        [[-0.1955,  0.4135,  0.0048,  0.7767,  0.5062,  1.2176,  0.5187,\n",
       "          -0.7878, -1.2677, -0.6992]],\n",
       "\n",
       "        [[-0.2435, -0.3629,  1.2211,  2.8558,  1.2008,  0.5523, -0.4086,\n",
       "           2.0123, -1.2107, -0.2737]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
