{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Central to all neural networks in PyTorch is the __autograd__ package.\n",
    "\n",
    "__autograd.Variable__ is something can be updated and auto grad, like W and b in neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "require_grad = True means this variable require backward, so b.grad_fn is an object, otherwise,\n",
    "b.grad_fn is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward>) <AddBackward object at 0x7f903f16e978>\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward>) grad of b1 is None\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2,requires_grad=True)\n",
    "print(a)\n",
    "b = a + 2\n",
    "print(b, b.grad_fn)\n",
    "\n",
    "a1 = torch.ones(2,2,requires_grad=True)\n",
    "b1 = a1 + 2\n",
    "print(b, \"grad of b1 is {}\".format(b1.grad_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Variable.backward() to compute the derivation of the Variable\n",
    "\n",
    "Here __b, c, out__ are all variable\n",
    "\n",
    ".backward() can be used because out is scala.\n",
    "\n",
    "运行这个 使得神经网络 从此处开始回传， 将此参数一直往前求导\n",
    "\n",
    "例如下例：\n",
    "\n",
    "If Variable is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to backward(), however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]]) tensor(27.)\n"
     ]
    }
   ],
   "source": [
    "c = b*b*3\n",
    "out = c.mean()\n",
    "print(c,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run __out.backward()__ will get each deraviation of out\n",
    "\n",
    "$\\frac{\\partial out}{\\partial c}$,\n",
    "$\\frac{\\partial out}{\\partial b}$,\n",
    "$\\frac{\\partial out}{\\partial a}$,\n",
    "\n",
    "### when run a.grad or b.grad or c.grad, it should gives the result of \n",
    "$\\frac{\\partial out}{\\partial a}$  or  $\\frac{\\partial out}{\\partial b}$ or $\\frac{\\partial out}{\\partial c}$,\n",
    "\n",
    "but as link below says:\n",
    "\n",
    "### hook media https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94\n",
    "gradients are only retained for leaf variables. non-leaf variables’ gradients are not retained to be inspected later. This was done by design, to save memory.\n",
    "\n",
    "So you can not get b.grad and c.grad by default.\n",
    "\n",
    "But we can use hook to get this:\n",
    "\n",
    "You should have got a matrix of ``4.5``. Let’s call the ``out``\n",
    "*Variable* “$o$”.\n",
    "We have that $o = \\frac{1}{4}\\sum_i c_i$,\n",
    "$c_i = 3(a_i+2)^2$ and $c_i\\bigr\\rvert_{a_i=1} = 27$.\n",
    "Therefore,\n",
    "$\\frac{\\partial o}{\\partial a_i} = \\frac{3}{2}(a_i+2)$, hence\n",
    "\n",
    "here, a1, a2, a3, a4 =1\n",
    "$\\frac{\\partial o}{\\partial a_i}\\bigr\\rvert_{a_i=1} = \\frac{9}{2} = 4.5$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6f31dc8ccc8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a.grad still be Variable!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But when run b.grad, c.grad then them is None.\n",
    "### Because pytorch only retain gradients of leaf variable to save memory, say \n",
    "b = a + 1\n",
    "\n",
    "c = $b * b * 3$\n",
    "\n",
    "out = c.mean()\n",
    "\n",
    "### only a is leaf variable. If we need inter-variable gradient, we need to use hook as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n"
     ]
    }
   ],
   "source": [
    "print(b.grad, c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 6\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      " 18\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "xx = Variable(torch.ones(1,1), requires_grad = True)\n",
    "yy = 3*xx\n",
    "zz = yy**2\n",
    "\n",
    "yy.register_hook(print)\n",
    "\n",
    "zz.backward()\n",
    "print(xx.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two more things!\n",
    "#### 1. When run .backward twice, it will give error, since when the graph back ward compute complete, they delete intermediate result, if you want to run it again, need to initial Variable again\n",
    "#### 2. If you didn't clear gradient of one variable, then run .backward() again, in former example, run out.backward() again, then x.grad will increase to 9!\n",
    "\n",
    "### Important!!\n",
    "#### In previous example, only scala can use .backward()\n",
    "#### actually, every Variable can use that, but need to pass in a tensor to make sure which part and relative weight for this part you selected to compute gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2  4  6  8\n",
      "[torch.FloatTensor of size 1x4]\n",
      " Variable containing:\n",
      " 20\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      " 2  0  0  0\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "\n",
      " 0  2  0  0\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "\n",
      " 2  2  2  2\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "\n",
      " 2  2  2  2\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "x = Variable(torch.FloatTensor([[1, 2, 3, 4]]), requires_grad=True)\n",
    "z = 2*x\n",
    "loss = z.sum(dim=1)\n",
    "print(z, loss)\n",
    "\n",
    "# do backward for first element of z\n",
    "z.backward(torch.FloatTensor([[1, 0, 0, 0]]))\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_() #remove gradient in x.grad, or it will be accumulated\n",
    "\n",
    "# do backward for second element of z\n",
    "z.backward(torch.FloatTensor([[0, 1, 0, 0]]))\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_()\n",
    "\n",
    "# do backward for all elements of z, with weight equal to the derivative of\n",
    "# loss w.r.t z_1, z_2, z_3 and z_4\n",
    "z.backward(torch.FloatTensor([[1, 1, 1, 1]]))\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_()\n",
    "\n",
    "# or we can directly backprop using loss\n",
    "loss.backward() # equivalent to loss.backward(torch.FloatTensor([1.0]))\n",
    "print(x.grad.data)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
