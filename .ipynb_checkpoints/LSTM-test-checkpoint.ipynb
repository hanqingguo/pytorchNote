{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.randn(1, 5) for _ in range(4)]  # make a sequence of length 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __4__ inputs, each input has __5__ features, so LSTM __input dim = 5__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.7585,  0.9087,  1.2925, -0.9285,  0.2344]]),\n",
       " tensor([[-0.2671, -0.8850, -2.1523, -1.6194, -1.7567]]),\n",
       " tensor([[ 0.3331,  1.6464, -0.0294,  0.9305,  0.4814]]),\n",
       " tensor([[-0.8014, -0.0726, -0.9579,  1.3642, -0.5175]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape = torch.cat(inputs).view(len(inputs),1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.7585,  0.9087,  1.2925, -0.9285,  0.2344]],\n",
       " \n",
       "         [[-0.2671, -0.8850, -2.1523, -1.6194, -1.7567]],\n",
       " \n",
       "         [[ 0.3331,  1.6464, -0.0294,  0.9305,  0.4814]],\n",
       " \n",
       "         [[-0.8014, -0.0726, -0.9579,  1.3642, -0.5175]]]),\n",
       " torch.Size([4, 1, 5]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape, reshape.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 is hidden_size, is hidden state dim.\n",
    "\n",
    "In this case, \n",
    "\n",
    "for each second, \n",
    "\n",
    "### xt.shape = (features, 1) => (5, 1)\n",
    "\n",
    "### yt.shape = (features, 1) => (5, 1)\n",
    "\n",
    "### ht.shape = (hidden_size, 1) => (3 ,1) \n",
    "\n",
    "### whh.shape = (hidden_size, hidden_size) =>(3,3)\n",
    "\n",
    "### wxh.shape = (hidden_size, features) => (3,5)\n",
    "\n",
    "### why.shape = (features, hidden_size) => (5,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For each element in the input sequence, each layer computes the following function__\n",
    "\n",
    "\\begin{split}\\begin{array}{ll}\n",
    "i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
    "c_t = f_t c_{(t-1)} + i_t g_t \\\\\n",
    "h_t = o_t \\tanh(c_t)\n",
    "\\end{array}\\end{split}\n",
    "\n",
    "![1.png](lstm-img/lstm1.png)\n",
    "![2.png](lstm-img/lstm2.png)\n",
    "![3.png](lstm-img/lstm3.png)\n",
    "![4.png](lstm-img/lstm4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(5,3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __input_size__ – The number of expected features in the input x\n",
    "* __hidden_size__ – The number of features in the hidden state h\n",
    "* __num_layers__ – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1 \n",
    "\n",
    "(__this is how many layers stack upper together__)\n",
    "\n",
    "* __bias__ – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "* __batch_first__ – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "* __dropout__ – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "* __bidirectional__ – If True, becomes a bidirectional LSTM. Default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1: initialize hidden\n",
    "hidden is tuple, \n",
    "(h0,c0)\n",
    "In this case, both h_0 and c_0 are (2,1,3)\n",
    "\n",
    "                                    (\n",
    "                                    \n",
    "                                     2  => lstm_num_layer * num_directions \n",
    "                                     1  => mini-batch_size,\n",
    "                                     3  => hidden_size\n",
    "                                     \n",
    "                                     )\n",
    "__num_direction__ is 1 by default (when bidirectional is False)\n",
    "\n",
    "__num_direction__ is 2 (when bidirectional is True)\n",
    "\n",
    "thanks answer from https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks#/media/File:RNN_BRNN.png\n",
    "\n",
    "https://discuss.pytorch.org/t/what-is-num-directions-of-nn-lstm/11663"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = (torch.randn(2, 1, 3),\n",
    "          torch.randn(2, 1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2: construct input\n",
    "input of lstm is (4,1,5)\n",
    "\n",
    "                 (\n",
    "                 \n",
    "                  4 => sequence_num,\n",
    "                  1 => mini-batch_size,\n",
    "                  5 => features number for every time\n",
    "                  \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = torch.cat(inputs).view(len(inputs),1,-1)\n",
    "input1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before inputs is:\n",
      " [tensor([[ 1.7585,  0.9087,  1.2925, -0.9285,  0.2344]]), tensor([[-0.2671, -0.8850, -2.1523, -1.6194, -1.7567]]), tensor([[ 0.3331,  1.6464, -0.0294,  0.9305,  0.4814]]), tensor([[-0.8014, -0.0726, -0.9579,  1.3642, -0.5175]])]\n",
      "\n",
      "before length of inputs is: 4\n",
      "\n",
      "after input1 is:\n",
      " tensor([[[ 1.7585,  0.9087,  1.2925, -0.9285,  0.2344]],\n",
      "\n",
      "        [[-0.2671, -0.8850, -2.1523, -1.6194, -1.7567]],\n",
      "\n",
      "        [[ 0.3331,  1.6464, -0.0294,  0.9305,  0.4814]],\n",
      "\n",
      "        [[-0.8014, -0.0726, -0.9579,  1.3642, -0.5175]]])\n",
      "\n",
      "after input1.size is: torch.Size([4, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"before inputs is:\\n {}\\n\\n\"\n",
    "      \"before length of inputs is: {}\\n\\n\"\n",
    "      \"after input1 is:\\n {}\\n\\n\"\n",
    "      \"after input1.size is: {}\".format(inputs, len(inputs), input1, input1.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input of lstm is:\n",
    "\n",
    "__input1, (h0, c0)__\n",
    "\n",
    "### Output of lstm is:\n",
    "\n",
    "__output, (hn, cn)__\n",
    "\n",
    "\n",
    "#### If input is only one step of a sequence, then ouput is\n",
    "\n",
    "__output__ of lstm is (1,1,3)\n",
    "\n",
    "                 (\n",
    "                \n",
    "                  1 => sequence_num,\n",
    "                  1 => mini-batch_size,\n",
    "                  3 => num_directions * hidden_size\n",
    "                  \n",
    "                 )\n",
    "              \n",
    "__num_direction__ is 1 by default (when bidirectional is False)\n",
    "\n",
    "__num_direction__ is 2 (when bidirectional is True)\n",
    "\n",
    "output __hidden__ is __h_n__, __c_n__,\n",
    "\n",
    "both of them have shape of (2,1,3)\n",
    "\n",
    "                                (\n",
    "\n",
    "                                 2  => lstm_num_layer * num_directions \n",
    "                                 1  => mini-batch_size,\n",
    "                                 3  => hidden_size\n",
    "\n",
    "                                 )\n",
    "                                 \n",
    "the last slice of __h_n__, is equal to __out__\n",
    "\n",
    "hidden[0][1,:,:] == out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out is:\n",
      "tensor([[[ 0.0486, -0.0038, -0.1808]]])\n",
      " out.shape is:\n",
      "torch.Size([1, 1, 3])\n",
      "\n",
      "\n",
      "h_n is:\n",
      "tensor([[[ 0.1133,  0.0932, -0.0258]],\n",
      "\n",
      "        [[ 0.0486, -0.0038, -0.1808]]])\n",
      "\n",
      ", h_n shape is:\n",
      "torch.Size([2, 1, 3])\n",
      "c_n is:\n",
      "tensor([[[ 0.1691,  0.1439, -0.0691]],\n",
      "\n",
      "        [[ 0.1078, -0.0114, -0.5262]]])\n",
      "\n",
      ", c_n shape is:\n",
      "torch.Size([2, 1, 3])\n",
      "\n",
      "out is:\n",
      "tensor([[[ 0.1031, -0.0778, -0.0843]]])\n",
      " out.shape is:\n",
      "torch.Size([1, 1, 3])\n",
      "\n",
      "\n",
      "h_n is:\n",
      "tensor([[[ 0.0335, -0.1472, -0.1614]],\n",
      "\n",
      "        [[ 0.1031, -0.0778, -0.0843]]])\n",
      "\n",
      ", h_n shape is:\n",
      "torch.Size([2, 1, 3])\n",
      "c_n is:\n",
      "tensor([[[ 0.0408, -0.2807, -0.2694]],\n",
      "\n",
      "        [[ 0.2861, -0.2007, -0.2837]]])\n",
      "\n",
      ", c_n shape is:\n",
      "torch.Size([2, 1, 3])\n",
      "\n",
      "out is:\n",
      "tensor([[[ 0.1225, -0.0928, -0.0572]]])\n",
      " out.shape is:\n",
      "torch.Size([1, 1, 3])\n",
      "\n",
      "\n",
      "h_n is:\n",
      "tensor([[[-0.0245,  0.3293, -0.0703]],\n",
      "\n",
      "        [[ 0.1225, -0.0928, -0.0572]]])\n",
      "\n",
      ", h_n shape is:\n",
      "torch.Size([2, 1, 3])\n",
      "c_n is:\n",
      "tensor([[[-0.0572,  0.5126, -0.1620]],\n",
      "\n",
      "        [[ 0.3279, -0.2472, -0.1559]]])\n",
      "\n",
      ", c_n shape is:\n",
      "torch.Size([2, 1, 3])\n",
      "\n",
      "out is:\n",
      "tensor([[[ 0.1319, -0.1161, -0.0492]]])\n",
      " out.shape is:\n",
      "torch.Size([1, 1, 3])\n",
      "\n",
      "\n",
      "h_n is:\n",
      "tensor([[[ 0.0520,  0.2710, -0.1167]],\n",
      "\n",
      "        [[ 0.1319, -0.1161, -0.0492]]])\n",
      "\n",
      ", h_n shape is:\n",
      "torch.Size([2, 1, 3])\n",
      "c_n is:\n",
      "tensor([[[ 0.1325,  0.4206, -0.1596]],\n",
      "\n",
      "        [[ 0.3525, -0.3098, -0.1405]]])\n",
      "\n",
      ", c_n shape is:\n",
      "torch.Size([2, 1, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    # hidden is a tuple\n",
    "    print(\"out is:\\n{}\\n out.shape is:\\n{}\\n\\n\".format(out, out.size()))\n",
    "    print(\"h_n is:\\n{}\\n\\n, h_n shape is:\\n{}\\n\"\n",
    "           \"c_n is:\\n{}\\n\\n, c_n shape is:\\n{}\\n\".format(hidden[0],hidden[0].size(), hidden[1], hidden[1].size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If input is whole sequence, then ouput is\n",
    "\n",
    "__output__ of lstm is (4,1,3)\n",
    "\n",
    "                 (\n",
    "                \n",
    "                  4 => sequence_num,\n",
    "                  1 => mini-batch_size,\n",
    "                  3 => num_directions * hidden_size\n",
    "                  \n",
    "                 )\n",
    "\n",
    "output __hidden__ is __h_n__, __c_n__,\n",
    "\n",
    "both of them have shape of (2,1,3)\n",
    "\n",
    "                                (\n",
    "\n",
    "                                 2  => lstm_num_layer * num_directions \n",
    "                                 1  => mini-batch_size,\n",
    "                                 3  => hidden_size\n",
    "\n",
    "                                 )\n",
    "                                 \n",
    "The last slice of __out__ is equal to last slice __h_n__\n",
    "\n",
    "out[3,:,:] == hidden[0][1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out is :\n",
      "tensor([[[ 0.0678, -0.2624, -0.1539]],\n",
      "\n",
      "        [[ 0.0935, -0.1988, -0.0568]],\n",
      "\n",
      "        [[ 0.1031, -0.1867, -0.0445]],\n",
      "\n",
      "        [[ 0.1161, -0.1871, -0.0482]]])\n",
      "\n",
      ", out.size is torch.Size([4, 1, 3])\n",
      "hidden is :\n",
      "(tensor([[[ 0.0016,  0.3302, -0.1628]],\n",
      "\n",
      "        [[ 0.1161, -0.1871, -0.0482]]]), tensor([[[ 0.0041,  0.5225, -0.2216]],\n",
      "\n",
      "        [[ 0.3147, -0.5357, -0.1303]]]))\n",
      "\n",
      "\n",
      "last output is:\n",
      " tensor([[ 0.1161, -0.1871, -0.0482]])\n",
      "\n",
      "\n",
      "h_n in last layer is:\n",
      " tensor([[ 0.1161, -0.1871, -0.0482]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "\n",
    "# out[3,:,:] is same as hidden[0][1,:,:]\n",
    "\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "hidden = (torch.randn(2, 1, 3), torch.randn(2, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(input1, hidden)\n",
    "print(\"out is :\\n{}\\n\\n, out.size is {}\".format(out, out.size()))\n",
    "\n",
    "print(\"hidden is :\\n{}\\n\\n\".format(hidden))\n",
    "\n",
    "print(\"last output is:\\n {}\\n\\n\".format(out[3,:,:]))\n",
    "print(\"h_n in last layer is:\\n {}\\n\\n\".format(hidden[0][1,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:,-1,:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0678, -0.2624, -0.1539]],\n",
       "\n",
       "        [[ 0.0935, -0.1988, -0.0568]],\n",
       "\n",
       "        [[ 0.1031, -0.1867, -0.0445]],\n",
       "\n",
       "        [[ 0.1161, -0.1871, -0.0482]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
